# -*- coding: utf-8 -*-
"""
Backend API cho Chatbot D∆∞·ª£c li·ªáu C·ªï truy·ªÅn (Ki·∫øn tr√∫c CRAG - Hugging Face)
S·ª≠ d·ª•ng FastAPI v√† LangGraph.
"""
import os
from fastapi import FastAPI, HTTPException
# *** S·ª¨A L·ªñI: S·ª≠a 'pantic' th√†nh 'pydantic' ***
from pydantic import BaseModel, Field
from typing import List, Literal, Any

# LangChain components
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.schema import Document, StrOutputParser

# LangGraph components
from langgraph.graph import StateGraph, END, START
from typing_extensions import TypedDict

# --- Kh·ªüi t·∫°o ·ª©ng d·ª•ng FastAPI ---
app = FastAPI(
    title="API Chatbot D∆∞·ª£c li·ªáu (CRAG - Hugging Face)",
    description="API cho chatbot n√¢ng cao ch·ªâ s·ª≠ d·ª•ng Hugging Face.",
    version="2.4.0",
)

# --- ƒê·ªãnh nghƒ©a model cho request v√† response ---
class ChatRequest(BaseModel):
    query: str
    huggingface_api_key: str

class ChatResponse(BaseModel):
    generation: str
    documents: List[str]

# --- Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ Retriever ---
retriever = None

def load_retriever():
    global retriever
    jq_schema = '.[] | "T√™n v·ªã thu·ªëc: " + .name + ". Chi ti·∫øt: " + .detail + ". T√≥m t·∫Øt: " + .summaried'
    loader = JSONLoader(file_path='./merged_data.json', jq_schema=jq_schema, text_content=True)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectorstore = FAISS.from_documents(chunks, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    print("‚úÖ Retriever ƒë√£ s·∫µn s√†ng!")

@app.on_event("startup")
async def startup_event():
    print("üöÄ Server ƒëang kh·ªüi ƒë·ªông v√† chu·∫©n b·ªã d·ªØ li·ªáu...")
    load_retriever()

# --- ƒê·ªãnh nghƒ©a State v√† c√°c Node c·ªßa Graph ---
class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[Document]
    # *** C·∫¨P NH·∫¨T: Quay l·∫°i qu·∫£n l√Ω 1 m√¥ h√¨nh LLM duy nh·∫•t ***
    llm: Any

def retrieve_node(state):
    print("---NODE: RETRIEVE---")
    documents = retriever.invoke(state["question"])
    return {"documents": documents}

def grade_documents_node(state):
    print("---NODE: GRADE DOCUMENTS---")
    question = state["question"]
    documents = state["documents"]
    llm = state["llm"]

    system = "B·∫°n l√† ng∆∞·ªùi ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa t√†i li·ªáu v·ªõi c√¢u h·ªèi. Ch·ªâ tr·∫£ l·ªùi 'yes' n·∫øu t√†i li·ªáu li√™n quan, ng∆∞·ª£c l·∫°i tr·∫£ l·ªùi 'no'."
    prompt = ChatPromptTemplate.from_messages([
        ("system", system),
        ("human", "T√†i li·ªáu:\n\n{document}\n\nC√¢u h·ªèi: {question}"),
    ])
    grader = prompt | llm | StrOutputParser()
    
    filtered_docs = []
    for d in documents:
        score = grader.invoke({"question": question, "document": d.page_content})
        grade = score.strip().lower()
        if 'yes' in grade:
            print("---GRADE: RELEVANT---")
            filtered_docs.append(d)
        else:
            print("---GRADE: NOT RELEVANT---")
    return {"documents": filtered_docs}

def transform_query_node(state):
    print("---NODE: TRANSFORM QUERY---")
    question = state["question"]
    llm = state["llm"]

    system = "B·∫°n l√† ng∆∞·ªùi vi·∫øt l·∫°i c√¢u h·ªèi ƒë·ªÉ t·ªëi ∆∞u h√≥a cho t√¨m ki·∫øm web."
    prompt = ChatPromptTemplate.from_messages([
        ("system", system),
        ("human", "C√¢u h·ªèi ban ƒë·∫ßu: {question}\n\nH√£y t·∫°o ra m·ªôt c√¢u h·ªèi c·∫£i ti·∫øn:"),
    ])
    rewriter = prompt | llm | StrOutputParser()
    better_question = rewriter.invoke({"question": question})
    return {"question": better_question}

def web_search_node(state):
    print("---NODE: WEB SEARCH---")
    question = state["question"]
    documents = state["documents"]
    
    web_search_tool = DuckDuckGoSearchRun()
    web_results = web_search_tool.run(question)
    web_results_doc = Document(page_content=web_results)
    if documents is None:
        documents = []
    documents.append(web_results_doc)
    return {"documents": documents}

def generate_node(state):
    print("---NODE: GENERATE---")
    question = state["question"]
    documents = state["documents"]
    llm = state["llm"]
    
    prompt_template = PromptTemplate.from_template(
        "D·ª±a v√†o ng·ªØ c·∫£nh sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát.\n\nNg·ªØ c·∫£nh: {context}\n\nC√¢u h·ªèi: {question}\n\nC√¢u tr·∫£ l·ªùi:"
    )
    rag_chain = prompt_template | llm | StrOutputParser()
    generation = rag_chain.invoke({"context": documents, "question": question})
    return {"generation": generation}

def decide_to_generate_edge(state):
    print("---EDGE: DECIDE TO GENERATE---")
    if not state["documents"]:
        return "transform_query"
    else:
        return "generate"

# --- API Endpoint ƒë·ªÉ chat ---
@app.post("/chat")
async def chat_with_bot(request: ChatRequest):
    if not retriever:
        raise HTTPException(status_code=503, detail="H·ªá th·ªëng ch∆∞a s·∫µn s√†ng.")
    
    # *** C·∫¨P NH·∫¨T: Kh·ªüi t·∫°o 1 m√¥ h√¨nh LLM duy nh·∫•t ***
    llm = HuggingFaceEndpoint(
        repo_id="meta-llama/Llama-2-70b-hf",
        temperature=0.1,
        max_new_tokens=1024,
        repetition_penalty=1.2,
        huggingfacehub_api_token=request.huggingface_api_key
    )

    # X√¢y d·ª±ng Graph
    workflow = StateGraph(GraphState)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("grade_documents", grade_documents_node)
    workflow.add_node("transform_query", transform_query_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("generate", generate_node)
    
    workflow.add_edge(START, "retrieve")
    workflow.add_edge("retrieve", "grade_documents")
    workflow.add_conditional_edges("grade_documents", decide_to_generate_edge, {
        "transform_query": "transform_query",
        "generate": "generate"
    })
    workflow.add_edge("transform_query", "web_search")
    workflow.add_edge("web_search", "generate")
    workflow.add_edge("generate", END)

    crag_app = workflow.compile()
    
    try:
        # *** C·∫¨P NH·∫¨T: Truy·ªÅn 1 LLM duy nh·∫•t v√†o state ban ƒë·∫ßu ***
        inputs = {
            "question": request.query, 
            "llm": llm
        }
        final_state = crag_app.invoke(inputs)
        
        doc_contents = [doc.page_content for doc in final_state.get('documents', [])]
        
        return {
            "generation": final_state.get('generation', "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi."),
            "documents": doc_contents
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω c·ªßa graph: {e}")
